{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "949d06c1",
   "metadata": {},
   "source": [
    "# Assignment 2 - part 1\n",
    "\n",
    "## Feed forward network (multilayer perceptron)\n",
    "\n",
    "In this assignment you shall develop the complete training and evaluation pipeline for a fully connected feed forward network.\n",
    "This shall cover all the stages discussed in the course, starting from data preparation and finishing with model evaluation.\n",
    "You can (you should) use the full functionality of PyTorch and all its packages.\n",
    "\n",
    "You can write most of your code as standard python scripts and packages outside jupyter notebook.\n",
    "The calls to the functionality shall, however, be executed from this notebook (not command-line).\n",
    "All printouts images and comments should be displayed in this notebook.\n",
    "\n",
    "You shall use this framework to train (at least) 3 feed-forward neural networks and compare their performance:\n",
    "- first, use only linear layers and non-linearites of your choice. You shall decide on the depth and width of the layers as well as all other hyperparameters as you see fit.\n",
    "- second, use linear layers, non-linearities and drop-out\n",
    "- third, use linear layers, non-linearities,  drop-out and batch norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc59e24",
   "metadata": {},
   "source": [
    "### Model training and evaluation\n",
    "\n",
    "Define the function `mlp_train` for training and evaluating an MLP model for classification of **FashionMNIST** data.\n",
    "The function shall be flexible so that it can take in all necessary hyper-parameters for the training. You shall not fix the hyper-parameters in the code of the function itself as fixed values.\n",
    "\n",
    "The `mlp_train` function shall return \n",
    "* the trained model `mlp_model`\n",
    "* anything else you deem important or useful for monitoring purposes etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87818299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.18.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: torch==2.3.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (2.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.3.1->torchvision) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.3.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.3.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.3.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.3.1->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.3.1->torchvision) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==2.3.1->torchvision) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchvision) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torchvision) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch==2.3.1->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch==2.3.1->torchvision) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643c71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA Loading\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define the transformation for the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the FashionMNIST dataset\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156e20f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Definition\n",
    "class BasicMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(BasicMLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        in_size = input_size\n",
    "        for h in hidden_sizes:\n",
    "            self.hidden_layers.append(nn.Linear(in_size, h))\n",
    "            in_size = h\n",
    "        self.output_layer = nn.Linear(in_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        for layer in self.hidden_layers:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "class DropoutMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(DropoutMLP, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input tensor\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class BatchNormMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, dropout_rate):\n",
    "        super(BatchNormMLP, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input tensor\n",
    "        return self.network(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b798308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "589af4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.5212, Train Accuracy: 0.8087, Test Loss: 0.4568, Test Accuracy: 0.8309\n",
      "Epoch [2/20], Train Loss: 0.3770, Train Accuracy: 0.8614, Test Loss: 0.3856, Test Accuracy: 0.8586\n",
      "Epoch [3/20], Train Loss: 0.3387, Train Accuracy: 0.8764, Test Loss: 0.3917, Test Accuracy: 0.8578\n",
      "Epoch [4/20], Train Loss: 0.3116, Train Accuracy: 0.8858, Test Loss: 0.3437, Test Accuracy: 0.8770\n",
      "Epoch [5/20], Train Loss: 0.2907, Train Accuracy: 0.8921, Test Loss: 0.3471, Test Accuracy: 0.8717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(basic_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m---> 15\u001b[0m basic_model, basic_train_losses, basic_test_losse  \u001b[38;5;241m=\u001b[39m \u001b[43mmlp_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 2. Dropout MLP\u001b[39;00m\n\u001b[0;32m     19\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\Downloads\\Assignment2ann\\cnn_code\\helpers.py:64\u001b[0m, in \u001b[0;36mmlp_train\u001b[1;34m(model, train_loader, test_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     61\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     62\u001b[0m all_preds, all_labels \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     65\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     66\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n\u001b[1;32m--> 172\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define function mlp_train so that it can be run from this cell\n",
    "from cnn_code.helpers import mlp_train\n",
    "\n",
    "# Training and Evaluating Models\n",
    "input_size = 28 * 28\n",
    "hidden_sizes = [256, 128, 64]\n",
    "output_size = 10\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "\n",
    "# 1. Basic MLP\n",
    "basic_model = BasicMLP(input_size, hidden_sizes, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(basic_model.parameters(), lr=learning_rate)\n",
    "basic_model, basic_train_losses, basic_test_losse  = mlp_train(basic_model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "\n",
    "# 2. Dropout MLP\n",
    "dropout_rate = 0.5\n",
    "dropout_model = DropoutMLP(input_size, hidden_sizes, output_size, dropout_rate)\n",
    "optimizer = optim.Adam(dropout_model.parameters(), lr=learning_rate)\n",
    "dropout_model, dropout_train_losses, dropout_test_losses = mlp_train(dropout_model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "\n",
    "# 3. BatchNorm + Dropout MLP\n",
    "batchnorm_model = BatchNormMLP(input_size, hidden_sizes, output_size, dropout_rate)\n",
    "optimizer = optim.Adam(batchnorm_model.parameters(), lr=learning_rate)\n",
    "batchnorm_model, batchnorm_train_losses, batchnorm_test_losses, batchnorm_accuracies = mlp_train(batchnorm_model, train_loader, test_loader, criterion, optimizer, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51fe2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "388b64cd",
   "metadata": {},
   "source": [
    "### Model application\n",
    "\n",
    "Define a simple utility function `mlp_apply` that uses the train model to classify 10 examples of the test set and displays the 10 images in a grid together with their true and predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617dc41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decd3420",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'basic_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# user parameters\u001b[39;00m\n\u001b[0;32m      5\u001b[0m test_indexes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m9\u001b[39m]  \u001b[38;5;66;03m# list of 10 indexes - examples to extract from test set\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m mlp_apply(\u001b[43mbasic_model\u001b[49m, test_loader, test_indexes)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# mlp_apply(mlp_model, test_indexes)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'basic_model' is not defined"
     ]
    }
   ],
   "source": [
    "# define function mlp_train so that it can be run from this cel\n",
    "from cnn_code.helpers import mlp_apply\n",
    "\n",
    "# user parameters\n",
    "test_indexes = [0,1,2,3,4,5,6,7,8,9]  # list of 10 indexes - examples to extract from test set\n",
    "mlp_apply(basic_model, test_loader, test_indexes)\n",
    "# mlp_apply(mlp_model, test_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e233f",
   "metadata": {},
   "source": [
    "### All experiments for getting high accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3443e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe947cec",
   "metadata": {},
   "source": [
    "### Train and apply model\n",
    "\n",
    "Use your functions defined above to train the three models. Try different values of the hyper-paramter settings. You shall achieve at least 80% test accuracy with all your models and at least 90% test accuracy with the best one.\n",
    "\n",
    "Describe briefly your three models and your hyper-parameter setups and comment your results.\n",
    "\n",
    "**Compare the performance of the three models using suitable supportive tables and graphs, and complemented by relevant comments.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d39568e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2241e979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/35], Train Loss: 0.5045, Train Accuracy: 0.8146, Test Loss: 0.4280, Test Accuracy: 0.8430\n",
      "Epoch [2/35], Train Loss: 0.3713, Train Accuracy: 0.8638, Test Loss: 0.3863, Test Accuracy: 0.8626\n",
      "Epoch [3/35], Train Loss: 0.3339, Train Accuracy: 0.8757, Test Loss: 0.3996, Test Accuracy: 0.8535\n",
      "Epoch [4/35], Train Loss: 0.3097, Train Accuracy: 0.8850, Test Loss: 0.3430, Test Accuracy: 0.8777\n",
      "Epoch [5/35], Train Loss: 0.2883, Train Accuracy: 0.8923, Test Loss: 0.3555, Test Accuracy: 0.8712\n",
      "Epoch [6/35], Train Loss: 0.2703, Train Accuracy: 0.8989, Test Loss: 0.3465, Test Accuracy: 0.8746\n",
      "Epoch [7/35], Train Loss: 0.2564, Train Accuracy: 0.9046, Test Loss: 0.3468, Test Accuracy: 0.8734\n",
      "Epoch [8/35], Train Loss: 0.2433, Train Accuracy: 0.9086, Test Loss: 0.3406, Test Accuracy: 0.8782\n",
      "Epoch [9/35], Train Loss: 0.2306, Train Accuracy: 0.9137, Test Loss: 0.3712, Test Accuracy: 0.8726\n",
      "Epoch [10/35], Train Loss: 0.2167, Train Accuracy: 0.9179, Test Loss: 0.3557, Test Accuracy: 0.8817\n",
      "Epoch [11/35], Train Loss: 0.2085, Train Accuracy: 0.9216, Test Loss: 0.3429, Test Accuracy: 0.8849\n",
      "Epoch [12/35], Train Loss: 0.1983, Train Accuracy: 0.9248, Test Loss: 0.3490, Test Accuracy: 0.8838\n",
      "Epoch [13/35], Train Loss: 0.1865, Train Accuracy: 0.9292, Test Loss: 0.3772, Test Accuracy: 0.8866\n",
      "Epoch [14/35], Train Loss: 0.1820, Train Accuracy: 0.9310, Test Loss: 0.3752, Test Accuracy: 0.8870\n",
      "Epoch [15/35], Train Loss: 0.1730, Train Accuracy: 0.9345, Test Loss: 0.3779, Test Accuracy: 0.8837\n",
      "Epoch [16/35], Train Loss: 0.1612, Train Accuracy: 0.9387, Test Loss: 0.3568, Test Accuracy: 0.8932\n",
      "Epoch [17/35], Train Loss: 0.1552, Train Accuracy: 0.9415, Test Loss: 0.3752, Test Accuracy: 0.8912\n",
      "Epoch [18/35], Train Loss: 0.1496, Train Accuracy: 0.9430, Test Loss: 0.3641, Test Accuracy: 0.8952\n",
      "Epoch [19/35], Train Loss: 0.1414, Train Accuracy: 0.9457, Test Loss: 0.3794, Test Accuracy: 0.8905\n",
      "Epoch [20/35], Train Loss: 0.1332, Train Accuracy: 0.9486, Test Loss: 0.4258, Test Accuracy: 0.8926\n",
      "Epoch [21/35], Train Loss: 0.1291, Train Accuracy: 0.9509, Test Loss: 0.4175, Test Accuracy: 0.8931\n",
      "Epoch [22/35], Train Loss: 0.1236, Train Accuracy: 0.9530, Test Loss: 0.4742, Test Accuracy: 0.8850\n",
      "Epoch [23/35], Train Loss: 0.1211, Train Accuracy: 0.9535, Test Loss: 0.4388, Test Accuracy: 0.8925\n",
      "Epoch [24/35], Train Loss: 0.1170, Train Accuracy: 0.9565, Test Loss: 0.4634, Test Accuracy: 0.8907\n",
      "Epoch [25/35], Train Loss: 0.1107, Train Accuracy: 0.9579, Test Loss: 0.4733, Test Accuracy: 0.8931\n",
      "Epoch [26/35], Train Loss: 0.1071, Train Accuracy: 0.9596, Test Loss: 0.4991, Test Accuracy: 0.8892\n",
      "Epoch [27/35], Train Loss: 0.0996, Train Accuracy: 0.9615, Test Loss: 0.4816, Test Accuracy: 0.8853\n",
      "Epoch [28/35], Train Loss: 0.0979, Train Accuracy: 0.9627, Test Loss: 0.5366, Test Accuracy: 0.8867\n",
      "Epoch [29/35], Train Loss: 0.0956, Train Accuracy: 0.9637, Test Loss: 0.5420, Test Accuracy: 0.8879\n",
      "Epoch [30/35], Train Loss: 0.0908, Train Accuracy: 0.9658, Test Loss: 0.5077, Test Accuracy: 0.8892\n",
      "Epoch [31/35], Train Loss: 0.0909, Train Accuracy: 0.9650, Test Loss: 0.5318, Test Accuracy: 0.8879\n",
      "Epoch [32/35], Train Loss: 0.0908, Train Accuracy: 0.9655, Test Loss: 0.5656, Test Accuracy: 0.8918\n",
      "Epoch [33/35], Train Loss: 0.0862, Train Accuracy: 0.9678, Test Loss: 0.5333, Test Accuracy: 0.8905\n",
      "Epoch [34/35], Train Loss: 0.0842, Train Accuracy: 0.9682, Test Loss: 0.5288, Test Accuracy: 0.8902\n",
      "Epoch [35/35], Train Loss: 0.0804, Train Accuracy: 0.9698, Test Loss: 0.6193, Test Accuracy: 0.8920\n",
      "Epoch [1/35], Train Loss: 0.6159, Train Accuracy: 0.7750, Test Loss: 0.4422, Test Accuracy: 0.8361\n",
      "Epoch [2/35], Train Loss: 0.4608, Train Accuracy: 0.8377, Test Loss: 0.4082, Test Accuracy: 0.8500\n",
      "Epoch [3/35], Train Loss: 0.4270, Train Accuracy: 0.8493, Test Loss: 0.4060, Test Accuracy: 0.8554\n",
      "Epoch [4/35], Train Loss: 0.4061, Train Accuracy: 0.8569, Test Loss: 0.3889, Test Accuracy: 0.8579\n",
      "Epoch [5/35], Train Loss: 0.3898, Train Accuracy: 0.8599, Test Loss: 0.3831, Test Accuracy: 0.8623\n",
      "Epoch [6/35], Train Loss: 0.3778, Train Accuracy: 0.8644, Test Loss: 0.3633, Test Accuracy: 0.8668\n",
      "Epoch [7/35], Train Loss: 0.3688, Train Accuracy: 0.8685, Test Loss: 0.3560, Test Accuracy: 0.8692\n",
      "Epoch [8/35], Train Loss: 0.3583, Train Accuracy: 0.8716, Test Loss: 0.3541, Test Accuracy: 0.8738\n",
      "Epoch [9/35], Train Loss: 0.3521, Train Accuracy: 0.8726, Test Loss: 0.3536, Test Accuracy: 0.8729\n",
      "Epoch [10/35], Train Loss: 0.3430, Train Accuracy: 0.8764, Test Loss: 0.3468, Test Accuracy: 0.8767\n",
      "Epoch [11/35], Train Loss: 0.3408, Train Accuracy: 0.8772, Test Loss: 0.3493, Test Accuracy: 0.8749\n",
      "Epoch [12/35], Train Loss: 0.3335, Train Accuracy: 0.8801, Test Loss: 0.3436, Test Accuracy: 0.8793\n",
      "Epoch [13/35], Train Loss: 0.3285, Train Accuracy: 0.8801, Test Loss: 0.3380, Test Accuracy: 0.8765\n",
      "Epoch [14/35], Train Loss: 0.3245, Train Accuracy: 0.8839, Test Loss: 0.3356, Test Accuracy: 0.8816\n",
      "Epoch [15/35], Train Loss: 0.3163, Train Accuracy: 0.8859, Test Loss: 0.3403, Test Accuracy: 0.8800\n",
      "Epoch [16/35], Train Loss: 0.3136, Train Accuracy: 0.8867, Test Loss: 0.3408, Test Accuracy: 0.8786\n",
      "Epoch [17/35], Train Loss: 0.3132, Train Accuracy: 0.8888, Test Loss: 0.3293, Test Accuracy: 0.8830\n",
      "Epoch [18/35], Train Loss: 0.3094, Train Accuracy: 0.8889, Test Loss: 0.3468, Test Accuracy: 0.8757\n",
      "Epoch [19/35], Train Loss: 0.3077, Train Accuracy: 0.8905, Test Loss: 0.3364, Test Accuracy: 0.8815\n",
      "Epoch [20/35], Train Loss: 0.3022, Train Accuracy: 0.8906, Test Loss: 0.3376, Test Accuracy: 0.8810\n",
      "Epoch [21/35], Train Loss: 0.2958, Train Accuracy: 0.8927, Test Loss: 0.3323, Test Accuracy: 0.8833\n",
      "Epoch [22/35], Train Loss: 0.2940, Train Accuracy: 0.8937, Test Loss: 0.3354, Test Accuracy: 0.8829\n",
      "Epoch [23/35], Train Loss: 0.2943, Train Accuracy: 0.8931, Test Loss: 0.3272, Test Accuracy: 0.8832\n",
      "Epoch [24/35], Train Loss: 0.2891, Train Accuracy: 0.8944, Test Loss: 0.3347, Test Accuracy: 0.8799\n",
      "Epoch [25/35], Train Loss: 0.2864, Train Accuracy: 0.8970, Test Loss: 0.3274, Test Accuracy: 0.8836\n",
      "Epoch [26/35], Train Loss: 0.2858, Train Accuracy: 0.8965, Test Loss: 0.3283, Test Accuracy: 0.8816\n",
      "Epoch [27/35], Train Loss: 0.2793, Train Accuracy: 0.8982, Test Loss: 0.3305, Test Accuracy: 0.8843\n",
      "Epoch [28/35], Train Loss: 0.2794, Train Accuracy: 0.8987, Test Loss: 0.3218, Test Accuracy: 0.8866\n",
      "Epoch [29/35], Train Loss: 0.2772, Train Accuracy: 0.8998, Test Loss: 0.3241, Test Accuracy: 0.8864\n",
      "Epoch [30/35], Train Loss: 0.2791, Train Accuracy: 0.8991, Test Loss: 0.3216, Test Accuracy: 0.8889\n",
      "Epoch [31/35], Train Loss: 0.2707, Train Accuracy: 0.9024, Test Loss: 0.3263, Test Accuracy: 0.8880\n",
      "Epoch [32/35], Train Loss: 0.2731, Train Accuracy: 0.9022, Test Loss: 0.3357, Test Accuracy: 0.8861\n",
      "Epoch [33/35], Train Loss: 0.2685, Train Accuracy: 0.9029, Test Loss: 0.3205, Test Accuracy: 0.8896\n",
      "Epoch [34/35], Train Loss: 0.2663, Train Accuracy: 0.9034, Test Loss: 0.3231, Test Accuracy: 0.8883\n",
      "Epoch [35/35], Train Loss: 0.2667, Train Accuracy: 0.9033, Test Loss: 0.3205, Test Accuracy: 0.8890\n",
      "Epoch [1/35], Train Loss: 0.5538, Train Accuracy: 0.8078, Test Loss: 0.4207, Test Accuracy: 0.8443\n",
      "Epoch [2/35], Train Loss: 0.4239, Train Accuracy: 0.8488, Test Loss: 0.3950, Test Accuracy: 0.8571\n",
      "Epoch [3/35], Train Loss: 0.3900, Train Accuracy: 0.8583, Test Loss: 0.3550, Test Accuracy: 0.8710\n",
      "Epoch [4/35], Train Loss: 0.3656, Train Accuracy: 0.8689, Test Loss: 0.3531, Test Accuracy: 0.8701\n",
      "Epoch [5/35], Train Loss: 0.3481, Train Accuracy: 0.8735, Test Loss: 0.3396, Test Accuracy: 0.8760\n",
      "Epoch [6/35], Train Loss: 0.3333, Train Accuracy: 0.8780, Test Loss: 0.3239, Test Accuracy: 0.8817\n",
      "Epoch [7/35], Train Loss: 0.3201, Train Accuracy: 0.8835, Test Loss: 0.3194, Test Accuracy: 0.8817\n",
      "Epoch [8/35], Train Loss: 0.3132, Train Accuracy: 0.8848, Test Loss: 0.3115, Test Accuracy: 0.8853\n",
      "Epoch [9/35], Train Loss: 0.3039, Train Accuracy: 0.8894, Test Loss: 0.3187, Test Accuracy: 0.8855\n",
      "Epoch [10/35], Train Loss: 0.2934, Train Accuracy: 0.8933, Test Loss: 0.3082, Test Accuracy: 0.8889\n",
      "Epoch [11/35], Train Loss: 0.2832, Train Accuracy: 0.8951, Test Loss: 0.3111, Test Accuracy: 0.8885\n",
      "Epoch [12/35], Train Loss: 0.2758, Train Accuracy: 0.8980, Test Loss: 0.3086, Test Accuracy: 0.8883\n",
      "Epoch [13/35], Train Loss: 0.2709, Train Accuracy: 0.9003, Test Loss: 0.3103, Test Accuracy: 0.8864\n",
      "Epoch [14/35], Train Loss: 0.2626, Train Accuracy: 0.9029, Test Loss: 0.3000, Test Accuracy: 0.8923\n",
      "Epoch [15/35], Train Loss: 0.2567, Train Accuracy: 0.9057, Test Loss: 0.2961, Test Accuracy: 0.8943\n",
      "Epoch [16/35], Train Loss: 0.2521, Train Accuracy: 0.9070, Test Loss: 0.2905, Test Accuracy: 0.8942\n",
      "Epoch [17/35], Train Loss: 0.2463, Train Accuracy: 0.9090, Test Loss: 0.2927, Test Accuracy: 0.8944\n",
      "Epoch [18/35], Train Loss: 0.2419, Train Accuracy: 0.9101, Test Loss: 0.2898, Test Accuracy: 0.8976\n",
      "Epoch [19/35], Train Loss: 0.2364, Train Accuracy: 0.9130, Test Loss: 0.2982, Test Accuracy: 0.8942\n",
      "Epoch [20/35], Train Loss: 0.2318, Train Accuracy: 0.9138, Test Loss: 0.2906, Test Accuracy: 0.8963\n",
      "Epoch [21/35], Train Loss: 0.2288, Train Accuracy: 0.9149, Test Loss: 0.2956, Test Accuracy: 0.8970\n",
      "Epoch [22/35], Train Loss: 0.2210, Train Accuracy: 0.9183, Test Loss: 0.2862, Test Accuracy: 0.9009\n",
      "Epoch [23/35], Train Loss: 0.2201, Train Accuracy: 0.9174, Test Loss: 0.2817, Test Accuracy: 0.8995\n",
      "Epoch [24/35], Train Loss: 0.2137, Train Accuracy: 0.9196, Test Loss: 0.2849, Test Accuracy: 0.9001\n",
      "Epoch [25/35], Train Loss: 0.2088, Train Accuracy: 0.9221, Test Loss: 0.2860, Test Accuracy: 0.9013\n",
      "Epoch [26/35], Train Loss: 0.2057, Train Accuracy: 0.9239, Test Loss: 0.2874, Test Accuracy: 0.9006\n",
      "Epoch [27/35], Train Loss: 0.2023, Train Accuracy: 0.9244, Test Loss: 0.2915, Test Accuracy: 0.9024\n",
      "Epoch [28/35], Train Loss: 0.1988, Train Accuracy: 0.9257, Test Loss: 0.2843, Test Accuracy: 0.9023\n",
      "Epoch [29/35], Train Loss: 0.1973, Train Accuracy: 0.9260, Test Loss: 0.3062, Test Accuracy: 0.8950\n",
      "Epoch [30/35], Train Loss: 0.1926, Train Accuracy: 0.9288, Test Loss: 0.2906, Test Accuracy: 0.8987\n",
      "Epoch [31/35], Train Loss: 0.1920, Train Accuracy: 0.9282, Test Loss: 0.2919, Test Accuracy: 0.9026\n",
      "Epoch [32/35], Train Loss: 0.1875, Train Accuracy: 0.9293, Test Loss: 0.2849, Test Accuracy: 0.9033\n",
      "Epoch [33/35], Train Loss: 0.1853, Train Accuracy: 0.9313, Test Loss: 0.2935, Test Accuracy: 0.9008\n",
      "Epoch [34/35], Train Loss: 0.1826, Train Accuracy: 0.9324, Test Loss: 0.2905, Test Accuracy: 0.9037\n",
      "Epoch [35/35], Train Loss: 0.1825, Train Accuracy: 0.9324, Test Loss: 0.2966, Test Accuracy: 0.8982\n"
     ]
    }
   ],
   "source": [
    "# define function mlp_train so that it can be run from this cell\n",
    "from cnn_code.helpers import mlp_train\n",
    "\n",
    "# Training and Evaluating Models\n",
    "input_size = 28 * 28\n",
    "hidden_sizes = [512,256, 128]\n",
    "output_size = 10\n",
    "num_epochs = 35\n",
    "learning_rate = 0.001\n",
    "dropout_rate = 0.35\n",
    "\n",
    "# 1. Basic MLP\n",
    "basic_model = BasicMLP(input_size, hidden_sizes, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(basic_model.parameters(), lr=learning_rate)\n",
    "basic_model, basic_train_losses, basic_test_losse, basic_train_accuracy,basic_test_accuracy   = mlp_train(basic_model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "\n",
    "# 2. Dropout MLP\n",
    "dropout_model = DropoutMLP(input_size, hidden_sizes, output_size, dropout_rate)\n",
    "optimizer = optim.Adam(dropout_model.parameters(), lr=learning_rate)\n",
    "dropout_model, dropout_train_losses, dropout_test_losses,dropout_train_accuracy,dropout_test_accuracy = mlp_train(dropout_model, train_loader, test_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "\n",
    "# 3. BatchNorm + Dropout MLP\n",
    "batchnorm_model = BatchNormMLP(input_size, hidden_sizes, output_size, dropout_rate)\n",
    "optimizer = optim.Adam(batchnorm_model.parameters(), lr=learning_rate)\n",
    "batchnorm_model, batchnorm_train_losses, batchnorm_test_losses,batchnorm_train_accuracy,batchnorm_test_accuracy= mlp_train(batchnorm_model, train_loader, test_loader, criterion, optimizer, num_epochs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
